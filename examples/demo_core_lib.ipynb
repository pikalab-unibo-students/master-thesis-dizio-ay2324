{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Demo Notebook for Intelligent System Engineering (ISE)",
   "id": "cac08e96ad4dac28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))"
   ],
   "id": "20bea5db3650cd9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import openml\n",
    "\n",
    "import fairlib as fl\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ],
   "id": "3610820f95d660a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading and Preparing the Adult Dataset.\n",
    "We will use the Adult dataset from OpenML, which contains demographic information and predicts whether an individual earns more than $50K per year."
   ],
   "id": "bb9ce5a128dc1dd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "adult_dataset = openml.datasets.get_dataset(179)\n",
    "adult_dataset, _, _, _ = adult_dataset.get_data(dataset_format=\"dataframe\")\n",
    "\n",
    "adult_dataset.rename(columns={'class': 'income'}, inplace=True)\n",
    "adult_dataset.drop(columns=[\"fnlwgt\"], inplace=True)"
   ],
   "id": "629df9e901a3bd68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Conversion of the pandas dataset to the Enhanced Dataset of Fairlib",
   "id": "9120f54006d923c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "adult = fl.DataFrame(adult_dataset)\n",
    "\n",
    "# Setting the target feature and sensitive attributes\n",
    "adult.targets = 'income'\n",
    "adult.sensitive = ['sex', 'race']"
   ],
   "id": "14779dbd07c048d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Metrics\n",
    "### Disparate Impact (DI)\n",
    "**Disparate impact** is a fairness metric in artificial intelligence that refers to indirect discrimination against protected groups (such as gender, race, etc.), even when sensitive attributes are not explicitly used by the model.\n",
    "\n",
    "It occurs when the likelihood of receiving a favorable outcome (e.g., loan approval) differs significantly between groups, violating the principle of equal treatment."
   ],
   "id": "2cc8b8442bb6c4d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "di = adult.disparate_impact()\n",
    "print(di)"
   ],
   "id": "14a7e7eb5a06e89f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"DI value for unprivileged group (such as female):\")\n",
    "print(di[{'income': \">50K\", 'sex': 'Male'}]) #"
   ],
   "id": "ce21d2389cf9bc8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Disparate Impact Value | Interpretation                                          | Group Benefiting                         |\n",
    "|------------------------|----------------------------------------------------------|------------------------------------------|\n",
    "| ❗ < 0.5                | **Extreme disparity** (strong likelihood of bias)        | Privileged group benefits very strongly  |\n",
    "| 0.5 – 0.8              | **Significant disparity** (likely bias)                  | Privileged group benefits strongly       |\n",
    "| 0.8 – 0.9              | **Moderate disparity** (potential bias)                  | Privileged group benefits                |\n",
    "| 0.9 – 1.0              | **Minimal disparity** (slight bias)                      | Privileged group benefits slightly       |\n",
    "| 1.0                    | **Perfect fairness** (equal impact for both groups)      | Neither (equal outcomes)                 |\n",
    "| 1.0 – 1.1              | **Minimal disparity** (slight bias)                      | Unprivileged group benefits slightly     |\n",
    "| 1.1 – 1.2              | **Moderate disparity** (potential bias)                  | Unprivileged group benefits              |\n",
    "| 1.2 – 1.5              | **Significant disparity** (likely bias)                  | Unprivileged group benefits strongly     |\n",
    "| > 1.5                  | **Extreme disparity** (strong likelihood of bias)        | Unprivileged group benefits very strongly|"
   ],
   "id": "7eae422e5aa401ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Statistical Parity Difference (SPD)\n",
    "**Statistical Parity Difference (SPD)** is a fairness metric that quantifies the difference in the probability of receiving a favorable outcome between a protected group and a reference (unprotected) group.\n",
    "It reflects whether individuals from different groups are equally likely to receive positive predictions, regardless of their actual qualifications or features."
   ],
   "id": "ceefc862209e4db7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "spd = adult.statistical_parity_difference()\n",
    "print(spd)"
   ],
   "id": "aaf4fcb9369a5a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"SPD value for unprivileged group (such as female):\")\n",
    "print(spd[{'income': \">50K\", 'sex': 'Male'}])"
   ],
   "id": "e2f4603705434b6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| SPD Value        | Interpretation                                    | Group Benefiting                         |\n",
    "|------------------|----------------------------------------------------|------------------------------------------|\n",
    "| > 0.2            | **Significant disparity** (likely bias)           | Privileged group benefits strongly       |\n",
    "| ❗ 0.1 – 0.2 | **Moderate disparity** (potential bias)           | Privileged group benefits                |\n",
    "| 0.01 – 0.1       | **Minimal disparity** (slight bias)               | Privileged group benefits slightly       |\n",
    "| 0                | **Perfect fairness** (equal treatment)           | Neither (equal outcomes)                 |\n",
    "| -0.01 – -0.1     | **Minimal disparity** (slight bias)               | Unprivileged group benefits slightly     |\n",
    "| -0.1 – -0.2      | **Moderate disparity** (potential bias)           | Unprivileged group benefits              |\n",
    "| < -0.2           | **Significant disparity** (likely bias)           | Unprivileged group benefits strongly     |"
   ],
   "id": "5f1b75e3f2ea2210"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset Preparation\n",
    "After performing a preliminary analysis, the algorithms can be used.\n",
    "This requires a dataset preparation phase."
   ],
   "id": "17639a8f492d2cb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Continue by using a single sensitive field to apply the binary classification algorithms\n",
    "adult.sensitive = ['sex']\n",
    "\n",
    "label_maps = {}\n",
    "\n",
    "for col in adult.columns:\n",
    "    if adult[col].dtype == 'object' or adult[col].dtype == 'category':\n",
    "        adult[col], uniques = pd.factorize(adult[col])\n",
    "        label_maps[col] = uniques\n",
    "\n",
    "print(f\"Dataset Form: {adult.shape}\")\n",
    "print(f\"Target Column: {adult.targets}\")\n",
    "print(f\"Sensitive Attributes: {adult.sensitive}\")\n",
    "\n",
    "adult.head()"
   ],
   "id": "c35c83daeba3aec8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(label_maps['sex']) # Male: 0, Female: 1\n",
    "print(label_maps['income']) # <=50K: 0, >50K: 1"
   ],
   "id": "486c7e7e905ed555"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sex_labels = label_maps['sex'].tolist()\n",
    "income_labels = label_maps['income'].tolist()\n",
    "\n",
    "counts = adult.groupby('sex')['income'].value_counts().unstack()\n",
    "\n",
    "counts.index = [sex_labels[i] for i in counts.index]\n",
    "counts.columns = [income_labels[i] for i in counts.columns]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "counts.plot(kind='bar', stacked=True)\n",
    "plt.title('Distribution of income by gender')\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Income')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "b1bdebc816cf1fd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = adult.drop(columns='income')\n",
    "y = adult['income']\n",
    "\n",
    "# Fairness information is maintained during dataframe operations\n",
    "print(f\"Sensitive Attributes: {X.sensitive}\")"
   ],
   "id": "a5b7b5f6b1fb0a37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.35, random_state=42\n",
    ")"
   ],
   "id": "c848cfa1ea0cedca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Demo Pre-processing\n",
    "Pre-processing algorithms aim to modify the dataset before training, this allows the training process not to be interfered with.\n",
    "The main problem with these algorithms is that they obfuscate features.\n",
    "\n",
    "Below we see an example, using the reweighing algorithm, which does not go to modify the features but generates weights that can be used in the training process to alert the algorithm to the presence of bias, leaving the features unaffected."
   ],
   "id": "55c7956043e94d68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_classifier(X_train, y_train, weight=None):\n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier with optional sample weights.\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clf.fit(X_train, y_train, sample_weight=weight)\n",
    "    return clf\n",
    "\n",
    "def evaluate_fairness(X_test, y_pred, positive_target=1, favored_class=0):\n",
    "    \"\"\"\n",
    "    Evaluate the fairness metrics (SPD and DI) of the predictions.\n",
    "    The positive_class and unfavored_class parameters allow you to specify\n",
    "    which target is considered positive and which is considered unfavored.\n",
    "    \"\"\"\n",
    "    X_test = X_test.copy()\n",
    "    X_test[\"income\"] = y_pred\n",
    "    dataset = fl.DataFrame(X_test)\n",
    "    dataset.targets = \"income\"\n",
    "    dataset.sensitive = \"sex\"\n",
    "\n",
    "    spd = dataset.statistical_parity_difference()[{'income': positive_target, 'sex': favored_class}]\n",
    "    di = dataset.disparate_impact()[{'income': positive_target, 'sex': favored_class}]\n",
    "    return spd, di"
   ],
   "id": "7aadc7f3742a0990"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To test the effectiveness of the algorithm, a classifier will be trained on the non-preprocessed dataset and then the same classifier will be trained on the preprocessed dataset.\n",
    "\n",
    "This way we can compare accuracy and correctness metrics."
   ],
   "id": "e2187f283f0a474e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train a baseline classifier without fairness preprocessing\n",
    "baseline_clf = train_classifier(X_train, y_train)\n",
    "\n",
    "# Evaluate the baseline model\n",
    "baseline_pred = baseline_clf.predict(X_test)\n",
    "baseline_accuracy = accuracy_score(y_test, baseline_pred)\n",
    "baseline_spd, baseline_di = evaluate_fairness(X_test, baseline_pred)\n",
    "\n",
    "print(f\"Baseline Model Accuracy: {baseline_accuracy:.4f}\")\n",
    "print(f\"Baseline Statistical Parity Difference: {baseline_spd}\")\n",
    "print(f\"Baseline Disparate Impact: {baseline_di}\")"
   ],
   "id": "ef56b9d11430a270"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from fairlib import Reweighing\n",
    "\n",
    "reweighing = Reweighing()\n",
    "reweighed_df = reweighing.fit_transform(adult)\n",
    "reweighed_df"
   ],
   "id": "8b63b12b89c0e0de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = reweighed_df.drop(columns='income')\n",
    "y = reweighed_df['income']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.35, random_state=42\n",
    ")\n"
   ],
   "id": "9f09c8abb2ba88f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "weights = X_train['weights']",
   "id": "c3214e140ab801fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train a baseline classifier without fairness preprocessing\n",
    "clf_trained_with_rew = train_classifier(X_train, y_train, weight=weights)\n",
    "\n",
    "# Evaluate the baseline model\n",
    "baseline_rew_pred = clf_trained_with_rew.predict(X_test)\n",
    "reweighed_accuracy = accuracy_score(y_test, baseline_rew_pred)\n",
    "reweighed_spd, reweighed_di = evaluate_fairness(X_test, baseline_rew_pred)\n",
    "\n",
    "print(f\"Baseline Model With Rew. Dataset Accuracy: {reweighed_accuracy:.4f}\")\n",
    "print(f\"Baseline Model With Rew. Dataset Statistical Parity Difference: {reweighed_spd}\")\n",
    "print(f\"Baseline Model With Rew. Disparate Impact: {reweighed_di}\")"
   ],
   "id": "2056ccb089a290ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Baseline', 'Reweighing'],\n",
    "    'Accuracy': [baseline_accuracy, reweighed_accuracy],\n",
    "    'SPD': [abs(baseline_spd), abs(reweighed_spd)],\n",
    "    'DI': [abs(baseline_di - 1), abs(reweighed_di - 1)]\n",
    "})\n",
    "\n",
    "print(\"Comparison of Models:\")\n",
    "print(results)\n",
    "\n",
    "_, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.bar(results['Model'], results['Accuracy'])\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.set_ylim(0.7, 0.9)\n",
    "\n",
    "# SPD comparison (lower is better)\n",
    "ax2.bar(results['Model'], results['SPD'])\n",
    "ax2.set_title('Statistical Parity Difference (lower is better)')\n",
    "ax2.set_ylim(0, 0.3)\n",
    "\n",
    "# DI comparison (lower is better)\n",
    "ax3.bar(results['Model'], results['DI'])\n",
    "ax3.set_title('Disparate Impact (lower is better)')\n",
    "ax3.set_ylim(0, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f771ced57b795569"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Demo In-Processing",
   "id": "f728082b111305de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "adult.drop(columns='weights', inplace=True)  # Remove weights column for in-processing",
   "id": "f8167896ac81c562"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split features and target\n",
    "X = adult.drop(columns=['income'])\n",
    "y = adult['income']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ],
   "id": "bbaa45bd486fcb97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "EPOCHS = 50"
   ],
   "id": "7381e95470aeb569"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To test the effectiveness of the algorithm, a model is created by specifying the parameter `lambda_adv` to 0. This allows obtaining a model that does not apply the principle of impartiality.\n",
    "\n",
    "Next, we proceed to train the same model by increasing the value of `lambda_adv` to 1. This adds a constraint to the model, increasing the impartiality."
   ],
   "id": "eb3a91e657d9120f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from fairlib import AdversarialDebiasing\n",
    "\n",
    "baseline_model = AdversarialDebiasing(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dim=8,\n",
    "    output_dim=1,\n",
    "    sensitive_dim=1,\n",
    "    lambda_adv=0, # No fairness intervention, baseline model\n",
    ")\n",
    "\n",
    "baseline_model.fit(X_train, y_train, num_epochs=EPOCHS)\n",
    "y_pred = baseline_model.predict(X_test).detach().cpu().numpy()"
   ],
   "id": "ae40c06502e73dba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "baseline_accuracy = accuracy_score(y_test.values, y_pred)\n",
    "baseline_spd, baseline_di = evaluate_fairness(X_test, y_pred)"
   ],
   "id": "48c650614eec5dcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Baseline model accuracy: {baseline_accuracy}\")\n",
    "print(f\"Statistical Parity Difference (SPD): {baseline_spd}\")\n",
    "print(f\"Disparate Impact (DI): {baseline_di}\")"
   ],
   "id": "83d601405f0543f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Application of Adversarial Debiasing",
   "id": "bd4f4fde494a1059"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from fairlib import AdversarialDebiasing\n",
    "\n",
    "fair_model = AdversarialDebiasing(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dim=8,\n",
    "    output_dim=1,\n",
    "    sensitive_dim=1,\n",
    "    lambda_adv=1, # Fairness intervention\n",
    ")\n",
    "\n",
    "fair_model.fit(X_train, y_train, num_epochs=EPOCHS)\n",
    "y_pred = fair_model.predict(X_test).detach().cpu().numpy()"
   ],
   "id": "2db410164621e17e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "adv_accuracy = accuracy_score(y_test.values, y_pred)\n",
    "adv_spd, adv_di = evaluate_fairness(X_test, y_pred)"
   ],
   "id": "80bad100f742d7b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Fair model accuracy: {adv_accuracy}\")\n",
    "print(f\"Statistical Parity Difference (SPD): {adv_spd}\")\n",
    "print(f\"Disparate Impact (DI): {adv_di}\")"
   ],
   "id": "1051b3b3f0f3d69b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Baseline', 'Adv Debiasing'],\n",
    "    'Accuracy': [baseline_accuracy, adv_accuracy],\n",
    "    'SPD': [abs(baseline_spd), abs(adv_spd)],\n",
    "    'DI': [abs(baseline_di - 1), abs(adv_di - 1)]\n",
    "})\n",
    "\n",
    "print(\"Comparison of Models:\")\n",
    "print(results)\n",
    "\n",
    "# Visualize the results\n",
    "_, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.bar(results['Model'], results['Accuracy'])\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.set_ylim(0.7, 0.9)\n",
    "\n",
    "# SPD comparison (lower is better)\n",
    "ax2.bar(results['Model'], results['SPD'])\n",
    "ax2.set_title('Statistical Parity Difference (lower is better)')\n",
    "ax2.set_ylim(0, 0.3)\n",
    "\n",
    "# DI comparison (lower is better)\n",
    "ax3.bar(results['Model'], results['DI'])\n",
    "ax3.set_title('Disparate Impact (lower is better)')\n",
    "ax3.set_ylim(0, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "4f816b76bfc0029d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n"
   ],
   "id": "275ddbf2fdd7f4fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
